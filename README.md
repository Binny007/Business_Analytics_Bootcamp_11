# Business_Analytics_Bootcamp_11


# ğŸ§ª Multivariate A/B Testing - Advanced Analytics

Dive deep into the world of data-driven decision making by mastering the art and science of Multivariate A/B Testing. Learn how companies like **Google** optimize user experience with statistical rigor and machine learning.

---

## ğŸ“ Repository Contents

| File Name                      | Description                                                        |
|-------------------------------|--------------------------------------------------------------------|
| `Multivariate_A_B_Testing.ipynb` | Interactive Python notebook exploring multivariate testing using real-world data |
| `ab_google.csv`               | Dataset simulating Google homepage variations and user engagement |
| `README.md`                   | This file â€“ your guide to the project structure and key takeaways |

---

## ğŸ§  Game Plan for Multivariate A/B Testing

A/B testing is no longer about just comparing two variants.  
This project evolves your toolkit from traditional A/B testing to **Multivariate Testing (MVT)** â€” enabling simultaneous testing of multiple design elements and their interactions.

Youâ€™ll:
- Identify multiple variables impacting user engagement
- Use statistical modeling and machine learning to test combinations
- Derive insights from untested variants through predictive modeling

---

## ğŸ•µï¸â€â™‚ï¸ Case Study: Google's Homepage Experiment

We simulate and analyze an experiment similar to one Google ran on its homepage â€” testing changes in:

- ğŸ” **Search Bar Shape**
- ğŸŸ  **Search Button Design**
- ğŸ¨ **Background Color**

Each user saw a unique combination. The dataset (`ab_google.csv`) helps us explore how subtle design tweaks can significantly impact **Session Duration**.

---

## ğŸ”¬ What is Multivariate A/B Testing (MVT)?

| A/B Test                        | Multivariate Test (MVT)                             |
|---------------------------------|-----------------------------------------------------|
| Tests one element at a time     | Tests multiple elements together                    |
| Simple to run                   | Complex due to interaction effects                  |
| Limited insight                 | Reveals combined impact of changes                  |

> MVT helps you find the **best-performing combination**, not just the best element.

---

## ğŸ”§ Full Factorial & Partial Factorial Testing

- **Full Factorial**: Tests *all* possible combinations (e.g., 2Ã—2Ã—2 = 8 versions)
- **Partial Factorial**: Reduces testing effort using smart experimental designs (e.g., Taguchi methods)

âœ… In this project, we use **full factorial design** to assess every combination of the three design elements.

---

## ğŸ“Š Exploratory Data Analysis (EDA)

We visualize and compare:

- ğŸ“ˆ Distribution of **Session Duration**
- ğŸ‘¥ Engagement across different **Search Bar**, **Button**, and **Background** styles
- ğŸ”„ Frequency of each design combination

This helps detect outliers, skewness, and preliminary performance differences.

---

## ğŸ“ˆ Regression & Random Forest Modeling in Python

We use machine learning to model how design changes affect user behavior.

### Highlights:
- **Linear Regression**: Great for understanding influence of each variable
- **Random Forest**: Captures **non-linear patterns** and **interactions**

ğŸ’¡ *Pro Tip*: You donâ€™t always need to add a constant to regression â€” especially in **tree-based models** like Random Forest!

---

## ğŸŒ² Random Forest Parameter Tuning

We fine-tuned hyperparameters to boost model performance:

| Parameter           | Description                          |
|---------------------|--------------------------------------|
| `n_estimators`      | Number of trees                      |
| `max_depth`         | Maximum depth of each tree           |
| `min_samples_split` | Minimum samples to split a node      |
| `min_samples_leaf`  | Minimum samples at a leaf node       |

âœ… Result: A **highly optimized model** to predict session durations.

---

## ğŸ§ª Hypothesis Testing

We conducted:

- âœ… **ANOVA**: To check if combinations significantly influence session duration
- âœ… **Tukeyâ€™s HSD Test**: To identify which specific pairs differ significantly

### Result:
- **p-value = 0.000** â†’ Statistically significant differences across design combinations  
- Many variations led to **significant improvements** in session duration

---

## ğŸ“Œ Final Analysis: Inference & Visualizations

- ğŸ“Š Visualized **average session duration per variant** using bar plots
- ğŸ” Compared **predicted vs. actual engagement**
- ğŸ¯ Identified **top-performing and underperforming** combinations

---

## ğŸš€ Biggest Takeaway: Predicting Untested Variants

Even if a design combo wasnâ€™t tested, ML models like **Random Forest** can **predict its performance**.

You're no longer limited to what was tested â€” now you can forecast the potential of unexplored variants and **prioritize smarter A/B testing**.

---

## ğŸ“¸ Sneak Peek

| Combination                  | Avg. Session Duration (sec) |
|------------------------------|-----------------------------|
| Original_Original_Original   | 2990.86                     |
| Rectangle_Star_Original      | 3409.63                     |
| Rectangle_Star_Light Pink    | 3387.57                     |

---

## ğŸ§  Interpreting Results in Python

Use this helper function in the notebook to interpret p-values:

```python
def interpret_p_value(p):
    if p < 0.05:
        return "âœ”ï¸ Reject Null Hypothesis"
    else:
        return "âŒ Fail to Reject Null Hypothesis"


```

## ğŸ“š Keywords for Discovery

  Multivariate AB Testing, Factorial Design, Google Homepage Experiment,
  Session Duration, Machine Learning, Random Forest, A/B Testing Analytics, User Behavior Modeling

## ğŸ¤ Letâ€™s Connect

If this repo adds value to your data science journey, consider giving it a â­ï¸ or raising an issue for suggestions!

Made with ğŸ§  and ğŸ in Python


